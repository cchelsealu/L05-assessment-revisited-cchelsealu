---
title: "L05 Assessment Revisited"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "YOUR NAME"

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false

from: markdown+emoji 
---

::: {.callout-tip icon=false}

## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://your-github-repo-url](https://your-github-repo-url)

:::

## Overview

The main goal of this lab is to have students think more about performance metrics, especially those used for classification.

## Exercises

### Exercise 1

When considering classification metric it is essential to understand the 4 essential terms below. Provide a definition for each.

::: {.callout-tip icon=false}

## Solution

- **True positives (TP):**

- **True negatives (TN):**

- **False positives (FP):**

- **False negatives (FN):**

:::

While the general definitions are useful it is vital to be able to interpret each in the context of a problem. Suppose we are attempting to classify an email as spam or not spam. We consider the prediction of spam to be a success or positive condition. 

Define each each of the terms and describe the consequence of each (what happens to the email) in the context of this problem:

::: {.callout-tip icon=false}

## Solution

- **True positives (TP):**

- **True negatives (TN):**

- **False positives (FP):**

- **False negatives (FN):**

:::

### Exercise 2

Using the email example again, suppose we are attempting to classify an email as spam or not spam. We consider the prediction of spam to be a success or positive condition.

Describe each of the metrics in context of our example and indicate how to use the metric (meaning is a lower or higher value better):

::: {.callout-tip icon=false}

## Solution

- **Accuracy:**

- **Precision:**

- **Recall:**

- **Sensitivity:**

- **Specificity:**

:::

### Exercise 3

Name one metric that you would use if you are trying to balance recall and precision. Also indicate how to read/use the metric to compare models.  

::: {.callout-tip icon=false}

## Solution


:::

### Exercise 4

Below is the code contained in a nearest neighbor tuning script, `knn_tuning.R`, that was used in lab 2. By default the accuracy and AUC performance metrics are calculated. Suppose that in addition to these two default performance metrics we wanted to also calculate precision, recall, sensitivity, specificity, and an F measure. Modify the script below so that all desired performance metrics are calculated during the tuning step. 

*Hint:* Start by defining the appropriate metric set and then reference that set within the function that carries out the tuning.

::: {.callout-tip icon=false}

## Solution

```{r}
#| label: knn-tuning
#| eval: false

# Knn tuning ----

# Load package(s) ----
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doMC)

# register cores/threads for parallel processing
registerDoMC(cores = parallel::detectCores(logical = TRUE))

# Handle conflicts
tidymodels_prefer()

# Seed
set.seed(2468)

# load required objects ----
load("model_info/tuning_setup.rda")

# Define model ----
knn_model <- nearest_neighbor(
  mode = "classification",
  neighbors = tune()
  ) %>%
  set_engine("kknn")

# # check tuning parameters
# hardhat::extract_parameter_set_dials(knn_model)

# set-up tuning grid ----
knn_params <- hardhat::extract_parameter_set_dials(knn_model) %>%
  update(neighbors = neighbors(range = c(1,40)))

# define grid
knn_grid <- grid_regular(knn_params, levels = 15)

# workflow ----
knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(wildfires_recipe)

# Tuning/fitting ----
knn_tune <- knn_workflow %>%
  tune_grid(
    resamples = wildfires_fold,
    grid = knn_grid,
    control = keep_pred
  )

# Write out results
save(knn_tune, file = "model_info/knn_tune.rda")
```

:::

There is some redundancy in the set of performance metrics we are using. What is it?

::: {.callout-tip icon=false}

## Solution

:::

### Exercise 5

When conducting regression ML we have used root mean squared error and R squared. In a few cases we made use of mean absolute error. 

Name at least 2 other regression performance metrics and their functions in our `tidymodels` framework. Provide a description of the metric and how to use it to understand a model's performance. 

::: {.callout-tip icon=false}

## Solution

:::

### Challenge
**Not Required**

It might be useful to have a baseline model for contextualizing and understanding if the models we are training are actually useful or an improvement. In many instances we are working on ML problems that do not have a known baseline or standard model to compare to so comparison to a null model is useful. Meaning the null model is a baseline model that we can use for comparison. Especially if we want to establish that models we are training are of any value (in terms of prediction). 

In the code chunk below provide the code necessary to define a null model in our `tidymodels` framework (model, engine, formula, mode) for both a regression and classification problem.

```{r}
# define null model for regression ML


# define null model for classification ML

```

