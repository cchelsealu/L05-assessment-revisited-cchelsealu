---
title: "L05 Assessment Revisited"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "Chelsea Lu"

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false

from: markdown+emoji 
---


::: {.callout-tip icon=false}

## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://github.com/cchelsealu/L05-assessment-revisited-cchelsealu.git](https://github.com/cchelsealu/L05-assessment-revisited-cchelsealu.git)

:::

## Overview

The main goal of this lab is to have students think more about performance metrics, especially those used for classification.

## Exercises

### Exercise 1

When considering classification metric it is essential to understand the 4 essential terms below. Provide a definition for each.

::: {.callout-tip icon=false}

## Solution

- **True positives (TP):**
When the prediction accurately predicts the outcome of interest.

- **True negatives (TN):**
When the prediction accurately predicts the negative class.

- **False positives (FP):**
When the prediction inaccurately predicts the outcome of interest.

- **False negatives (FN):**
When the prediction inaccurately predicts the negative class.

:::

While the general definitions are useful it is vital to be able to interpret each in the context of a problem. Suppose we are attempting to classify an email as spam or not spam. We consider the prediction of spam to be a success or positive condition. 

Define each each of the terms and describe the consequence of each (what happens to the email) in the context of this problem:

::: {.callout-tip icon=false}

## Solution

- **True positives (TP):**
We accurately predict an email to be spam, it is spam.

- **True negatives (TN):**
We accurately predict an email not to be spam, it is not spam. 

- **False positives (FP):**
We classify an email to be spam when it wasn't spam. 

- **False negatives (FN):**
We classify an email not to be spam when it was spam.

:::

### Exercise 2

Using the email example again, suppose we are attempting to classify an email as spam or not spam. We consider the prediction of spam to be a success or positive condition.

Describe each of the metrics in context of our example and indicate how to use the metric (meaning is a lower or higher value better):

::: {.callout-tip icon=false}

## Solution

- **Accuracy:** 
The ratio of accurately predicted emails to the total number of emails predicted. A higher accuracy value is better than a lower one

- **Precision:**
Determines how accurately the model predicted emails were spam. Of all the predictions to be labelled as spam (positive), which of those predictions were accurate (true positives). A higher precision value is better 

- **Recall:**
Determines that of all the emails that are actually spam, which emails were predicted to be spam. This is determined by finding the ratio of accurately predicted spam emails to accurately predicted spam emails and inaccurately predicted non-spam emails. A higher recall is better


- **Sensitivity:**
The same thing as recall but is the term used in medical settings 

- **Specificity:**
Determines that of all emails that are actually not spam, which emails were predicted to be be not spam. A higher specificity is better to avoid falsely categorizing emails as spam. 

:::

### Exercise 3

Name one metric that you would use if you are trying to balance recall and precision. Also indicate how to read/use the metric to compare models.  

::: {.callout-tip icon=false}

## Solution
You would use the F1 measure to balance recall and precision. Higher F1 score is better because the better it is at predicting performance, and the range is between 0 and 1 

:::

### Exercise 4

Below is the code contained in a nearest neighbor tuning script, `knn_tuning.R`, that was used in lab 2. By default the accuracy and AUC performance metrics are calculated. Suppose that in addition to these two default performance metrics we wanted to also calculate precision, recall, sensitivity, specificity, and an F measure. Modify the script below so that all desired performance metrics are calculated during the tuning step. 

*Hint:* Start by defining the appropriate metric set and then reference that set within the function that carries out the tuning.

::: {.callout-tip icon=false}

## Solution

```{r}
#| label: knn-tuning
#| eval: false

# Knn tuning ----

# Load package(s) ----
library(tidyverse)
library(tidymodels)
library(tictoc)
library(doMC)

# register cores/threads for parallel processing
registerDoMC(cores = parallel::detectCores(logical = TRUE))

# Handle conflicts
tidymodels_prefer()

# Seed
set.seed(2468)

# load required objects ----
load("model_info/tuning_setup.rda")

# Define model ----
knn_model <- nearest_neighbor(
  mode = "classification",
  neighbors = tune()
  ) %>%
  set_engine("kknn")

# # check tuning parameters
# hardhat::extract_parameter_set_dials(knn_model)

# set-up tuning grid ----
knn_params <- hardhat::extract_parameter_set_dials(knn_model) %>%
  update(neighbors = neighbors(range = c(1,40)))

# define grid
knn_grid <- grid_regular(knn_params, levels = 15)

# workflow ----
knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(wildfires_recipe)

# define metrics
knn_metrics <- metric_set(precision, recall, sens, spec, f_meas)

# Tuning/fitting ----
knn_tune <- knn_workflow %>%
  tune_grid(
    resamples = wildfires_fold,
    grid = knn_grid,
    control = keep_pred,
    metrics = knn_metrics
  )

# Write out results
save(knn_tune, file = "model_info/knn_tune.rda")
```

:::

There is some redundancy in the set of performance metrics we are using. What is it?

::: {.callout-tip icon=false}

## Solution
Sensitivity is redundant because it is the same as recall. Sensitivity is just used in medical settings.

:::

### Exercise 5

When conducting regression ML we have used root mean squared error and R squared. In a few cases we made use of mean absolute error. 

Name at least 2 other regression performance metrics and their functions in our `tidymodels` framework. Provide a description of the metric and how to use it to understand a model's performance. 

::: {.callout-tip icon=false}

## Solution
Adjusted r sqaure: adjR2() 
You want the value to be high because there is a higher amount of variance explained. This function is similar to the R squared function. However, it also takes into account the amount of predictors to measure performance. If you have different number of predictors in two recipes, r squared will just think the recipe with the most predictors to be the most accurate. 

 concordance correlation coefficient: ccc()
 This metric takes into account the correlation and the difference between two continuous variables. It's a measure of bivariate correlation. The higher value the better, and it ranges from -1 to 1. 1 indicates most agreement between two variables.

:::

